{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## I) Pré traitement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0) Récupération"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db = load_all_sentences();\n",
    "print('chargement de {} vers dans la db'.format(len(db.keys())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "base_artistes = defaultdict(set)\n",
    "for k,v in db.iteritems():\n",
    "    base_artistes[v['artistes']].add(k)\n",
    "artistes = { k:v for k,v in artistes.iteritems() if len(v) > 200 }\n",
    "print('{} artistes'.format(len(artistes)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Exploration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "test = \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\"\n",
    "\n",
    "nltk.word_tokenize(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(\"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def freq_stats_corpora():\n",
    "    corpora = defaultdict(list)\n",
    "\n",
    "    # Création d'un corpus de tokens par artiste\n",
    "    for artiste,sentence_id in artistes.iteritems():\n",
    "        for sentence_id in sentence_id:\n",
    "            corpora[artiste] += tokenizer.tokenize(\n",
    "                db[sentence_id]['text'].decode('utf-8').lower()\n",
    "            )\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v)}\n",
    "\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "# Récupération des comptages\n",
    "freq, stats, corpora = freq_stats_corpora()\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "\n",
    "# Affichage des fréquences\n",
    "df.sort(columns='total', ascending=False)\n",
    "df.plot(kind='bar', color=\"#f56900\", title='Top 50 Rappeurs par nombre de mots')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def freq_stats_corpora():\n",
    "    corpora = defaultdict(list)\n",
    "    for artiste,sentence_ids in artistes.iteritems():\n",
    "        for sentence_id in sentence_ids:\n",
    "            corpora[artiste] += tokenizer.tokenize(\n",
    "                db[sentence_id]['text'].decode('utf-8').lower()\n",
    "            )\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "\n",
    "    return (freq, stats, corpora)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) Nettoyage / Normalisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) supprimer les stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Premièrement, on récupère la fréquence totale de chaque mot sur tout le corpus d'artistes\n",
    "freq_totale = nltk.Counter()\n",
    "for k, v in corpora.iteritems():\n",
    "    freq_totale += freq[k]\n",
    "\n",
    "# Deuxièmement on décide manière un peu arbitraire du nombre de mots les plus fréquents à supprimer. On pourrait afficher un graphe d'évolution du nombre de mots pour se rendre compte et avoir une meilleure heuristique.\n",
    "most_freq = zip(*freq2.most_common(100))[0] # stopwords =\n",
    "\n",
    "# On créé notre set de stopwords final qui cumule ainsi les 100 mots les plus fréquents du corpus ainsi que l'ensemble de stopwords par défaut présent dans la librairie NLTK\n",
    "sw = set()\n",
    "sw.update(stopwords)\n",
    "sw.update(tuple(nltk.corpus.stopwords.words('french')))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def freq_stats_corpora2(lookup_table=[]):\n",
    "    corpora = defaultdict(list)\n",
    "    for artist, block_ids in lt_artists.iteritems():\n",
    "        for block_id in block_ids:\n",
    "            tokens = tokenizer.tokenize(db_flat[block_id]['text'].decode('utf-8'))\n",
    "            corpora[artist] += [w for w in tokens if not w in list(sw)]\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "freq2, stats2, corpora2 = freq_stats_corpora2()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) Lemmatisation ou racinisation (stemming)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Racinisation :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def freq_stats_corpora3(lookup_table=[]):\n",
    "    corpora = defaultdict(list)\n",
    "    for artist, block_ids in lt_artists.iteritems():\n",
    "        for block_id in block_ids:\n",
    "            tokens = tokenizer.tokenize(db_flat[block_id]['text'].decode('utf-8').lower())\n",
    "            corpora[artist] += [stemmer.stem(w) for w in tokens if not w in list(sw)]\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "freq3, stats3, corpora3 = freq_stats_corpora3()\n",
    "df3 = pd.DataFrame.from_dict(stats3, orient='index').sort(columns='unique', ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quelle est la forme du Dataframe ?\n",
    "\n",
    "Y a t-il des valeurs manquantes ou des valeurs dupliquées ?\n",
    "\n",
    "Quelles sont les colonnes qui vont nous intéresser ?\n",
    "\n",
    "Y a-t-il des données aberrantes ou des incohérences majeures dans les données ?\n",
    "\n",
    "Y a t-il des tweets anormalement longs / courts ? Peut-on les considérer comme des outliers ?\n",
    "\n",
    "Quel est le ratio tweet qui parlent de “catastrophes” / tweet normaux ?\n",
    "\n",
    "En regardant quelques tweets au hasard, peut-on deviner facilement la “target” ?\n",
    "\n",
    "Peut-on déjà détecter des “patterns” ou des mots clés dans les tweets?\n",
    "\n",
    "A votre avis quel serait l’accuracy score qu’un humain pourrait obtenir s’il prédisait  les données “à la main” ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) Text Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pouvez-vous écrire une fonction qui : tokenize un document, supprime les stopwords, supprime les tokens de moins de 3 lettres ?\n",
    "\n",
    "Comment peut-on reconstituer le corpus (c'est-à dire un texte avec l’ensemble des documents) ?\n",
    "\n",
    "Une fois ce corpus constitué, combien de tokens uniques le constitue? Ce nombre vous apparaît-il faible, important, gigantesque ?\n",
    "\n",
    "Comment réduire ce nombre de tokens uniques, ou autrement dit “comment réduire la taille du vocabulaire” de ce corpus ?\n",
    "\n",
    "Combien de tokens sont présents une seule fois ? Ces tokens nous seront-ils utiles ?\n",
    "\n",
    "Appliquer une méthode de stemmatisation ou de lemmatisation peut-elle nous aider à réduire la dimensionnalité du corpus ?\n",
    "\n",
    "Comment visualiser graphiquement, par un WordCloud par exemple, les tokens les plus présents ?\n",
    "\n",
    "Pouvez vous appliquer tous les traitements évoqués afin de créer une nouvelle colonne “text” qui serait plus pertinente ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# II) Transformations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) Bag of Words / n gram / TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token_dict[file] = no_punctuation\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=sw)\n",
    "values = tfidf.fit_transform(token_dict.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La matrice TF-IDF est définie pour chaque mot relativement à un corpus, comme le produit TF * IDF où:\n",
    "\n",
    "TF = nombre de fois où le mot est dans le document / nombre de mots dans le document\n",
    "IDF = nombre de documents / nombre de documents où apparaît le mot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) Words embeddings : Word2vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3) LDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Préttt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# TF : term frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Créer le modèle LDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n                          max_iter=5, n_components=20, random_state=0)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n                          max_iter=5, n_components=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n                          max_iter=5, n_components=20, random_state=0)</pre></div></div></div></div></div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer le modèle LDA\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=5,\n",
    "        learning_method='online',\n",
    "        learning_offset=50.,\n",
    "        random_state=0)\n",
    "\n",
    "# Fitter sur les données\n",
    "lda.fit(tf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['00', '000', '01', '02', '03', '04', '0d', '0t', '10', '100', '11',\n       '12', '128', '13', '14', '145', '15', '16', '17', '18', '19',\n       '1990', '1991', '1992', '1993', '1d9', '1st', '1t', '20', '200',\n       '21', '22', '23', '24', '25', '250', '26', '27', '28', '29', '2di',\n       '2tm', '30', '300', '31', '32', '33', '34', '34u', '35', '36',\n       '37', '38', '39', '3d', '3t', '40', '42', '43', '44', '45', '50',\n       '500', '55', '60', '64', '6ei', '70', '75', '75u', '7ey', '7u',\n       '80', '800', '86', '90', '91', '92', '93', '9v', 'a86', 'able',\n       'ac', 'accept', 'access', 'according', 'act', 'action', 'actually',\n       'add', 'addition', 'address', 'administration', 'advance', 'age',\n       'ago', 'agree', 'ah', 'air', 'al', 'algorithm', 'allow', 'allowed',\n       'alt', 'america', 'american', 'analysis', 'anonymous', 'answer',\n       'answers', 'anti', 'anybody', 'apparently', 'appears', 'apple',\n       'application', 'applications', 'appreciate', 'appreciated',\n       'approach', 'appropriate', 'apr', 'april', 'archive', 'area',\n       'aren', 'argument', 'armenia', 'armenian', 'armenians', 'arms',\n       'army', 'article', 'articles', 'ask', 'asked', 'asking', 'assume',\n       'atheism', 'attack', 'au', 'author', 'authority', 'available',\n       'average', 'avoid', 'away', 'ax', 'b8f', 'bad', 'base', 'based',\n       'basic', 'basically', 'basis', 'belief', 'believe', 'best',\n       'better', 'bh', 'bhj', 'bible', 'big', 'bike', 'bit', 'bits',\n       'black', 'block', 'blood', 'board', 'body', 'book', 'books',\n       'bought', 'box', 'break', 'bring', 'brought', 'btw', 'build',\n       'building', 'built', 'bus', 'business', 'buy', 'bxn', 'ca',\n       'cable', 'california', 'called', 'calls', 'came', 'canada', 'car',\n       'card', 'cards', 'care', 'carry', 'cars', 'case', 'cases', 'cause',\n       'cd', 'center', 'certain', 'certainly', 'chance', 'change',\n       'changed', 'changes', 'check', 'chicago', 'child', 'children',\n       'chip', 'chips', 'choice', 'christ', 'christian', 'christianity',\n       'christians', 'church', 'chz', 'citizens', 'city', 'claim',\n       'claims', 'class', 'clear', 'clearly', 'clinton', 'clipper',\n       'close', 'code', 'color', 'com', 'come', 'comes', 'coming',\n       'command', 'comments', 'commercial', 'common', 'communications',\n       'community', 'comp', 'company', 'complete', 'completely',\n       'computer', 'condition', 'conference', 'congress', 'consider',\n       'considered', 'contact', 'contains', 'context', 'continue',\n       'control', 'controller', 'copy', 'correct', 'cost', 'couldn',\n       'country', 'couple', 'course', 'court', 'cover', 'create',\n       'created', 'crime', 'cross', 'cs', 'current', 'currently', 'cut',\n       'cx', 'd9', 'data', 'date', 'dave', 'david', 'day', 'days', 'db',\n       'dc', 'dead', 'deal', 'death', 'decided', 'defense', 'define',\n       'deleted', 'department', 'des', 'design', 'designed', 'details',\n       'development', 'device', 'devices', 'did', 'didn', 'die',\n       'difference', 'different', 'difficult', 'digital', 'directly',\n       'directory', 'discussion', 'disk', 'display', 'distribution',\n       'division', 'dod', 'does', 'doesn', 'doing', 'don', 'dos', 'doubt',\n       'dr', 'drive', 'driver', 'drivers', 'drives', 'drug', 'early',\n       'earth', 'easily', 'east', 'easy', 'ed', 'edu', 'effect',\n       'electronic', 'email', 'encryption', 'end', 'enforcement',\n       'engine', 'entire', 'entry', 'environment', 'error', 'escrow',\n       'especially', 'event', 'events', 'evidence', 'exactly', 'example',\n       'excellent', 'exist', 'existence', 'exists', 'expect',\n       'experience', 'explain', 'export', 'extra', 'face', 'fact',\n       'faith', 'false', 'family', 'faq', 'far', 'fast', 'faster',\n       'father', 'fax', 'fbi', 'federal', 'feel', 'field', 'figure',\n       'file', 'files', 'final', 'finally', 'fine', 'firearms', 'floppy',\n       'folks', 'follow', 'following', 'food', 'force', 'form', 'format',\n       'free', 'freedom', 'friend', 'ftp', 'function', 'functions',\n       'future', 'g9v', 'game', 'games', 'gas', 'gave', 'general',\n       'generally', 'gets', 'getting', 'gif', 'given', 'gives', 'giving',\n       'giz', 'gk', 'gm', 'goal', 'god', 'goes', 'going', 'good', 'got',\n       'gov', 'government', 'graphics', 'great', 'greek', 'ground',\n       'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half', 'hand',\n       'happen', 'happened', 'happens', 'happy', 'hard', 'hardware',\n       'haven', 'having', 'head', 'health', 'hear', 'heard', 'held',\n       'hell', 'help', 'hi', 'high', 'higher', 'history', 'hit', 'hockey',\n       'hold', 'home', 'hope', 'hot', 'hours', 'house', 'hp', 'human',\n       'ibm', 'ide', 'idea', 'ideas', 'ii', 'image', 'images', 'imagine',\n       'important', 'include', 'included', 'includes', 'including',\n       'individual', 'info', 'information', 'input', 'inside',\n       'installed', 'instead', 'insurance', 'int', 'interested',\n       'interesting', 'interface', 'internal', 'international',\n       'internet', 'involved', 'isn', 'israel', 'israeli', 'issue',\n       'issues', 'jesus', 'jewish', 'jews', 'jim', 'job', 'jobs', 'john',\n       'jpeg', 'just', 'key', 'keyboard', 'keys', 'kill', 'killed',\n       'kind', 'knew', 'know', 'knowledge', 'known', 'knows', 'la',\n       'land', 'language', 'large', 'late', 'later', 'launch', 'law',\n       'laws', 'league', 'learn', 'leave', 'left', 'legal', 'let',\n       'letter', 'level', 'library', 'life', 'light', 'like', 'likely',\n       'limited', 'line', 'lines', 'list', 'little', 'live', 'living',\n       'lk', 'll', 'local', 'long', 'longer', 'look', 'looked', 'looking',\n       'looks', 'lord', 'lost', 'lot', 'lots', 'love', 'low', 'lower',\n       'ma', 'mac', 'machine', 'machines', 'mail', 'main', 'major',\n       'make', 'makes', 'making', 'man', 'manager', 'manual', 'mark',\n       'market', 'mass', 'material', 'matter', 'max', 'maybe', 'mb',\n       'mean', 'meaning', 'means', 'media', 'medical', 'members',\n       'memory', 'men', 'mention', 'mentioned', 'message', 'mike',\n       'military', 'million', 'mind', 'mit', 'mode', 'model', 'modem',\n       'money', 'monitor', 'month', 'months', 'moral', 'motif', 'mouse',\n       'mr', 'ms', 'multiple', 'nasa', 'national', 'nature', 'near',\n       'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new',\n       'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal',\n       'north', 'note', 'nsa', 'number', 'numbers', 'object', 'obvious',\n       'obviously', 'offer', 'office', 'official', 'oh', 'ok', 'old',\n       'ones', 'open', 'opinion', 'opinions', 'orbit', 'order', 'org',\n       'organization', 'original', 'os', 'output', 'outside', 'package',\n       'page', 'paper', 'particular', 'parts', 'party', 'past', 'paul',\n       'pay', 'pc', 'peace', 'people', 'perfect', 'performance', 'period',\n       'person', 'personal', 'phone', 'physical', 'pick', 'picture',\n       'pin', 'pittsburgh', 'pl', 'place', 'places', 'plan', 'play',\n       'player', 'players', 'plus', 'point', 'points', 'police', 'policy',\n       'political', 'population', 'port', 'position', 'possible',\n       'possibly', 'post', 'posted', 'posting', 'power', 'pp', 'present',\n       'president', 'press', 'pretty', 'previous', 'price', 'printer',\n       'privacy', 'private', 'pro', 'probably', 'problem', 'problems',\n       'process', 'product', 'products', 'program', 'programs', 'project',\n       'protect', 'provide', 'provided', 'provides', 'pts', 'pub',\n       'public', 'published', 'purpose', 'quality', 'question',\n       'questions', 'quite', 'radio', 'ram', 'range', 'rate', 'read',\n       'reading', 'real', 'really', 'reason', 'reasonable', 'reasons',\n       'received', 'recent', 'recently', 'record', 'red', 'reference',\n       'related', 'release', 'religion', 'religious', 'remember', 'reply',\n       'report', 'reported', 'reports', 'request', 'require', 'required',\n       'requires', 'research', 'response', 'rest', 'result', 'results',\n       'return', 'right', 'rights', 'road', 'rom', 'room', 'rules', 'run',\n       'running', 'runs', 'russian', 'safety', 'said', 'sale', 'san',\n       'satellite', 'save', 'saw', 'say', 'saying', 'says', 'school',\n       'sci', 'science', 'scientific', 'screen', 'scsi', 'season',\n       'second', 'secret', 'section', 'secure', 'security', 'seen',\n       'self', 'sell', 'send', 'sense', 'sent', 'serial', 'series',\n       'server', 'service', 'set', 'shall', 'shipping', 'short', 'shot',\n       'shuttle', 'similar', 'simple', 'simply', 'sin', 'single', 'site',\n       'sites', 'situation', 'size', 'sl', 'small', 'society', 'software',\n       'solution', 'son', 'soon', 'sorry', 'sort', 'sound', 'sounds',\n       'source', 'sources', 'south', 'soviet', 'space', 'special',\n       'specific', 'speed', 'st', 'standard', 'start', 'started', 'state',\n       'statement', 'states', 'station', 'stephanopoulos', 'steve',\n       'stop', 'story', 'street', 'strong', 'study', 'stuff', 'subject',\n       'suggest', 'sun', 'supply', 'support', 'supports', 'supposed',\n       'sure', 'switch', 'systems', 'taken', 'takes', 'taking', 'talk',\n       'talking', 'tape', 'tar', 'tax', 'team', 'teams', 'technical',\n       'technology', 'tell', 'term', 'terms', 'test', 'text', 'thank',\n       'thanks', 'theory', 'thing', 'things', 'think', 'thinking',\n       'thought', 'time', 'times', 'title', 'today', 'told', 'took',\n       'tools', 'total', 'trade', 'transfer', 'tried', 'trouble', 'true',\n       'truth', 'try', 'trying', 'turkey', 'turkish', 'turks', 'turn',\n       'tv', 'type', 'uk', 'understand', 'unfortunately', 'unit',\n       'united', 'university', 'unix', 'unless', 'usa', 'use', 'used',\n       'useful', 'usenet', 'user', 'users', 'uses', 'using', 'usually',\n       'value', 'values', 'van', 'various', 've', 'version', 'vga',\n       'video', 'view', 'voice', 'volume', 'vs', 'w7', 'wait', 'want',\n       'wanted', 'wants', 'war', 'washington', 'wasn', 'water', 'way',\n       'ways', 'weapons', 'week', 'weeks', 'went', 'white', 'wide',\n       'widget', 'willing', 'win', 'window', 'windows', 'wire', 'wish',\n       'wm', 'women', 'won', 'word', 'words', 'work', 'worked', 'working',\n       'works', 'world', 'worth', 'wouldn', 'write', 'writing', 'written',\n       'wrong', 'wrote', 'x11', 'xt', 'year', 'years', 'yes', 'york',\n       'young'], dtype=object)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "tf_feature_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Une alternative, NMF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Une autre type de modélisation de sujet automatique non supervisée est NMF (Negative Matrix Factorisation)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oumei\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\decomposition\\_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95,\n",
    "                                   min_df=2,\n",
    "                                   max_features=no_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}